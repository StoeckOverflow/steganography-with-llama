# Hiders

## Probability Order Hider
This concept is largely based on the blog post by user pokes, who proposes using the order of the LLM’s next token top predictions to encode information. In this method, the LLM is prompted with the beginning of a sentence and asked to return the *n* most likely candidates for the next token. The choice of candidate then encodes the information. For example, the sentence "I want some" could be used as a prompt, and the top candidates for the next token (in order) could be `[ice, chocolate, more, of]`. Pokes proposes using a Huffman Code as the bits to hide in the text (which vary in length to minimize mean code word length), but we have already encoded and compressed the secret using arithmetic encoding so we can be more efficient with the bits per token.

We take a slightly different approach:
- We still use the order of the predictions to encode the information, but we encode *2n* bits in the top *n* predictions of the model (*n* is a hyper-parameter that has to be known to both the encoder and the decoder).
- If the next bits that need to be hidden are all zeros, we take the first choice. If they are all zeros and the least significant bit is a one, we take the second prediction, and so on.
- The prediction is then added to the prompt and the same process is repeated until the whole secret has been encoded.

Decoding or restoring the message from a manipulated feed works in the exact same way. One needs to know what the prompt was (in our case we used a fixed number of words which is another tunable hyper-parameter that has to be known to both encoder and decoder) in order to repeat the process exactly as the hider did. Every time one checks what the index of the next token in the text is within the array of predictions, that index is translated to its binary representation and those are the next bits of the secret. We also need to know the length of the secret to know when all of it has been found so that we can stop and use Adaptive Arithmetic Coding to transform the float back into the sequence of base64 encoded characters, fully recovering the encoded secret.

## Arithmetic Encoding Hider (Multiple Secret Probability Order Hider)
By learning from the strengths and weaknesses of our Probability Order Hider, we made key upgrades that allow for the model to spread the secret in the whole news feed more uniformly, reducing the detectability of the modified feeds. However, we were not able to implement the whole planned pipeline, so the parts that were planned and designed but still had conceptual problems or bugs will be marked to differentiate them from the actual working version.

The main differences from this approach to its predecessor include:

- Working with tokens that are outside of the encoding distribution (i.e., if we’re using the top 2n predictions to encode n bits, any token that we expect to code information but falls outside of this range can be seen as a special kind of marker. By using these markers, we can also mark a separation between secrets, allowing us to encode more than one within the same news feed and by using the markers in a specific way (i.e., we used two markers in a row) we can signal that the whole hidden message has been found and stop retrieving, without needing any a priori knowledge about the length of the secret. Using markers allowed us to split the floating point number into three parts: the required accuracy in bits of the decimal representation, the exponent, and the mantissa of the binary representation (a sign bit is not needed because our range goes from 0 to 1).

- This method is able to encode almost 80% of the secret in a single article, of which we have 30. So, in order to spread the encoding of the secret, making the statistical features of the article resemble those of the original ones, we decided to add padding tokens in between each coding token. Padding tokens are skipped by the encoder and decoder, so instead of sampling them based on the secret we select the one that will make the resulting article have a similar distribution to the original text.

- There are two methods we tried for sampling the padding tokens. The simple one is simply taking the top prediction of the LLM, and the alternative is using a pseudo-random sequence to sample from the LLM’s predictions, so that the chosen tokens follow the same distribution as the ones in the original text. The second method, although it sounded promising, did not yield better results in practice.

- How many padding tokens to pack between coding tokens also became a hyper-parameter that the decoder needs to know in order to skip them. It is also important that the padding token generation is in sync in the encoder and decoder.

In order to determine the maximum amount of padding tokens that one can utilize, one needs to know how many coding tokens are needed, and approximately how many tokens will be available to encode information in the whole news feed. The encoder’s hyper-parameters that have to be known by the decoder are then:

1. **Number of prompt words:** How many words to use as a prompt in each case.
2. **Bits per coding token:** How many of the top LLM predictions are used for encoding.
3. **Number of padding tokens:** How many padding tokens are generated between each coding token.

We also made improvements to the post-processing of the logits, expanding it to not only prevent the model from sampling an EOS token too early, which crashes the pipeline, but also to only use the same tokens that were encoded in the original feeds. We achieved this by sampling all of the provided clean feeds and making a list of whitelisted tokens. When the log-likelihood of the tokens is computed, we set the value of all non-whitelisted tokens to be negative infinity. As an addition to the whitelist, we constructed an empirical blacklist that contained the tokens that would very often cause the LLM to start hallucinating.

Two more manual pre- and post-processing methods of the text were attempted but didn’t yield the expected good results. One method involved a logits processor that prioritizes tokens whose log probability is close to the entropy of the token probability distribution, aiming to yield very natural results. However, this approach was not effective with our model and was removed from the pipeline. The other discarded method was a manual attempt of replacing special characters that may have more than one Unicode representation before and after the encoding to ensure a more stable text generation. This method was later removed as it increased complexity without improving quality.

## Synonym Hider

While the Probability Order Hider is a generative steganography approach, the edit-based approach of the Synonym Hider is used by other teams and should therefore be considered for the engineering of a detector. Ueoka et al. formalized an edit-based synonym substitution method to embed steganography in a text utilizing a masked LLM model. In a text with some words replaced by `[MASK]`, the masked LLM learns to identify the original words using their context. It also predicts a probability distribution of each masked token based on the applicability of the token, hereafter referred to as the probability distribution vector. Given a masking strategy and an encoding strategy based on the probability distribution vector, steganography can be efficiently embedded into a news feed. Due to simplicity, we use the block encoding as in the paper. Words with probabilities greater than a specific threshold are selected and sorted, excluding words to be skipped. Unique bit chunks are assigned to each word based on the number of remaining words.

The main reason why we implemented this method is for dataset generation. We need a training dataset for the seekers, which is large and contains news feeds, where the binary secret is embedded through different types of steganography methods. This ensures an increased variety in the datasets, so that different patterns of steganography can be captured during training.

At the moment, our implemented method is not fully functional, as the decoding mechanism contains a problem with the masking strategy and encoding/decoding strategy. Our implementation, based on Ueoka et al.’s repository, assumes that the embedding of the secret uses every masked token of the news feed, but mostly the embedding of the secret requires fewer masked tokens. Despite that, the secret is encoded correctly in only the required tokens, but the decoding mechanism uses all masked tokens which leads to an error. A start and stop token mechanism could be integrated to solve this problem. Furthermore, we experienced a problem in the binary encoding and decoding. In certain situations of the decoding, we get a false number of leading zeros in the block-decoded tokens. The source of this error has not been completely identified at the moment.

After a short investigation, we found out that the probability distribution vector and decoding is equal to the encoding. Therefore, inference issues in the masked model can be excluded for the moment. Despite the errors, the generated steganographic feeds are sufficient for the training of the seeker, as we are only interested in the probabilistic characteristics. Therefore, we stopped the error investigation as our main goal for the implementation of this hider has been achieved.
