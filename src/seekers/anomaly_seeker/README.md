## ML-based Anomaly Detector

As the classification by zero-shot methods through simple statistical metrics did not deliver profound results, we advanced this approach by leveraging more sophisticated machine learning methods in combination with a feature set, better tailored to linguistic characteristics. The idea was that this advancement surpasses previous methodologies by delving into the assumption that texts with embedded steganographic content exhibit discernible deviations in statistical patterns and linguistic structures. The following components were used as the feature set:

1. **Linguistic Features:** Selected to gauge text complexity and readability, including metrics like average sentence length and Flesch Reading Ease scores. The latter directly assesses text accessibility based on word and sentence length, plus syllable count, serving as a quantitative tool to differentiate between standard articles and those potentially harboring hidden messages.

2. **Statistical Features:** Including entropy and perplexity to evaluate text predictability and distribution consistency. These indicators illuminate language's natural variation, flagging unusual word and phrase distributions suggestive of covert alterations. Entropy measures text randomness, while perplexity reflects predictive performance, both critical for spotting linguistic pattern deviations.

3. **LLM Probabilities:** We hypothesized that utilizing the deep linguistic comprehension of LLMs to assess token occurrence probabilities, would aid in the detection of steganographically modified texts through token distribution anomalies.

4. **Special Characters and Character Count:** Hypothesizing that alterations in the frequency and distribution of special characters, alongside overall character count, might signal text manipulation.

For an effective steganography classification, we experimented with various machine learning models, notably focusing on the Random Forest Classifier (RFC) and One-Class Support Vector Machine (OC-SVM), fine-tuned via RandomizedSearchCV to determine optimal hyperparameters. RandomizedSearchCV systematically searches through a predefined hyperparameter space using a randomized selection process, as opposed to exhaustively evaluating all possible combinations. This approach significantly enhances the efficiency of identifying the most effective hyperparameter settings for our models. The models were evaluated based on precision, recall, and F1 scores, with OC-SVM consistently outperforming RFC. In the training of our models, we opted not to utilize batch processing due to the relatively modest size of our feature set, which allowed for direct and efficient training. Instead, we implemented a basic train-test split methodology.

Addressing the challenge of discerning the most contributive features within our models, we employed permutation importance, a technique facilitated by the scikit-learn library, to demystify model explainability and feature impact. Permutation importance is a technique for measuring the importance of a feature in a prediction model by calculating the increase in the prediction error of the model after permutation of the feature values, thereby cancelling the relationship between the feature and the true outcome. This method illuminated the differential focus of our models: the SVM identified mean sentence length as a pivotal indicator of textual manipulation, while the RFC leaned towards perplexity and entropy as key discriminators. Notably, normalizing perplexity via z-score normalization based on the dataset's mean and standard deviation yielded marginal improvements. Although permutation importance highlights feature significance rather than classification quality, our experimentation with various feature combinations consistently showed that no alternative set surpassed the OC-SVM's performance, which prominently featured mean sentence length.

Furthermore, we experimented with statistical tests, more specifically the t-test, Anderson-Darling (AD-test), and Kolmogorov-Smirnov (KS-test), to assess the significance of all features aggregated across news feeds. Unfortunately, these methods did not provide clear distinctions between the metrics. The t-test aims to identify significant differences between the means of two groups, potentially linked to specific hypotheses. The Anderson-Darling test evaluates how well a sample distribution fits a specified distribution, while the Kolmogorov-Smirnov test compares two distributions to assess their similarity. Our goal was to identify the most promising features based on their F1-Score and use them to construct an ensemble classifier to reduce overfitting. However, this approach did not yield meaningful outcomes.

Additionally, the use of Large Language Models (LLMs) like BERT and LLaMA presented its own set of challenges. The fixed-size input vectors of these models limited our ability to tokenize and process entire texts as single entities, affecting the models' ability to fully understand articles within the context of a news feed. This limitation underscores the complexity of applying LLMs to analyze aggregated news feed data, highlighting the need for alternative approaches or adaptations to better accommodate the intricacies of such tasks.

Despite the Anomaly Detector's initial promise, it was ultimately not selected for our final detection suite due to critical evaluations of its performance and generalizability across varied datasets. Initially trained on a comprehensive Kaggle dataset to augment our original, limited dataset, the model showed moderate success, achieving an F1 score of 0.68 on the Kaggle test split but only 0.50 on the submission platform. Training exclusively on the provided dataset yielded perfect local test scores but failed to replicate this success in