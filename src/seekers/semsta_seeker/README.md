Certainly! Here's the content you provided formatted in Markdown:

---

### DL-based Detector leveraging Semantic and Statistical Features

Prompted by these insights and the limitations observed in both zero-shot methods and traditional machine learning models, we embarked on developing a more sophisticated technique. We chose a deep learning-based approach, inspired by the work of Guo et al., which incorporates both semantic and statistical analysis.

![Overview of the DL-based Approach with semantic (Bert-encoded) and statistical (TF-IDF vectorized) Feature Extraction, inspired by Guo et al.](resources/figures/overviewDL-BasedApproach.png)

Our system's architecture is composed of four main components:

1. **Feature Extraction:** This is a two-part process. Initially, BERT, known for its effectiveness in transfer learning, is used to extract semantic features from the text. Following this, we apply a TF-IDF vectorizer in combination with an Auto-Encoder architecture to ascertain the statistical properties of the text.
2. **Fusion Component:** The extracted features from both semantic and statistical analyses are then combined. This component integrates the diverse data into a unified information space.
3. **Classifier:** At the heart of our decision-making process lies a classifier enhanced by the Badhanau attention mechanism. It utilizes both the original BERT-embedded representation from the semantic feature extraction and the derived statistical representation. The classifier is then tasked with determining whether the news feed conceals steganographic content.

This approach enables a nuanced and deep understanding of the text, leveraging both language context and statistical significance to make informed decisions on the presence of steganographic messages.

#### Semantic Feature Extraction with BERT

In our quest to extract deep semantic features from text, we opted for BERT (more specifically *bert-base-uncased*) over Llama due to its accessibility via the Hugging Face transformers library, which offers numerous advantages. The Hugging Face platform's user-friendly interface significantly streamlines development by enabling quick loading and inference with BERT and other pre-trained models, offering rapid deployment across tasks without complex setup, a critical advantage in resource-constrained environments. In contrast, the Llama_cpp environment presented significant barriers to entry, including sparse documentation and a more limited feature set. The practical implications became evident when tokenizing text with Llama took approximately 15 minutes per news feed item, a delay that was unsustainable given our time constraints and added unwanted complexity to our operations.

While Llama was also compatible with the transformers library, its stringent hardware demands ultimately led us to choose BERT for our needs. Utilizing PyTorch's dynamic computation graph and Autograd feature, our deep learning architecture achieves optimized efficiency and simplified training, while its integration with the transformers library enhances our workflow from data preprocessing to model evaluation, accelerating the processing of large datasets.

BERT leverages the Transformer architecture, distinguished by multi-head attention layers and fully connected layers, creating a sophisticated neural network. This architecture is enhanced with dynamic word embeddings, allowing BERT to generate context-sensitive representations where the meaning of a word can shift based on surrounding words. BERT’s bidirectional training under the masked language model (MLM) approach furthers its ability to deeply understand sentence structure and meaning, predicting words in the context of their surrounding text. The residual connections within BERT prevent vanishing gradients, thus maintaining effective training over many layers.

When BERT processes a sentence, it starts by encoding the sequence of words into a high-dimensional space with its embedding layer, which synthesizes token embeddings, segment embeddings, and position embeddings for each word. This is where its attention mechanisms shine, weighting the influence of words relative to one another, and allowing for nuanced understanding of context and long-range dependencies. As a result, BERT captures the full spectrum of semantic nuances, making it adept at understanding complex sentence structures and the relationships between distant words. Especially in the case of steganography this is very important as secrets don’t have to be densely encoded into the text. BERT’s final layer's output is transformed into an information space via a dense layer and activated with a sigmoid function. This produces a confidence score for the semantic features, indicating the model's certainty about the extracted semantic information. This is the form of latent representation we use as input for the fusion component and as input for the classifier.

#### Statistical Feature Extraction with TF-IDF

Current linguistic steganography methods primarily utilize the LLMs with conditional probability coding. As this approach influences the statistical distribution of word probabilities within the text used for steganography, understanding the relevance of words in a document is crucial. To quantify this relevance, Guo et al. proposes to use the TF-IDF vectorizer (Term Frequency-Inverse Document Frequency). This measure consists of two parts: Term Frequency (TF) assesses the prevalence of a word within a single document, underpinning the notion that frequent terms are of greater significance to the document. The Inverse Document Frequency (IDF), on the other hand, gauges the rarity of a term across the entire collection of documents, or corpus, implying that common words

 are less indicative of a document's unique context. The power of TF-IDF lies in its ability to assign a weight to each word in a document, reflecting its importance in relation to the whole corpus. Thus, it illuminates words that are both common in a particular document but rare elsewhere, highlighting them as contextually significant. TF-IDF's ability to emphasize words unique to a document makes it an invaluable tool for feature extraction in steganalysis, where it excels over methods like CountVectorizer by providing a richer, context-sensitive data representation that enhances the detection of subtle text manipulations to conceal information and improves overall model performance.

We leverage Auto-Encoders, a specialized neural network architecture composed of an encoder and a decoder, to process and interpret TF-IDF vectors efficiently. The Auto-Encoder excels in unsupervised learning tasks by compacting high-dimensional data, such as our TF-IDF vectors, into a lower-dimensional, essential representation through its encoder. This process involves efficient feature extraction that distills the key information within the data, utilizing the neural network function \( H_A = \sigma(W \cdot TF-IDF_{i,j} + b) \), where \( W \) and \( b \) represent the weight and bias of the neuron, respectively, and \( \sigma \) denotes the activation function. Subsequently, the decoder reconstructs the input from this condensed form, aiming to preserve the original data's critical features.

Integrating Auto-Encoders for encoding TF-IDF vectors achieves a dual purpose. Firstly, it simplifies the management of complex, high-dimensional data by encapsulating the most informative aspects into a denser, more abstract representation. Secondly, this condensed form is vital for detecting patterns indicative of steganography within text. Integrating TF-IDF with Auto-Encoders, we significantly enhance our architecture's ability to discern between regular and steganographically altered text. This synergy combines TF-IDF's emphasis on keyword significance with Auto-Encoders' efficient feature compression, providing a robust statistical input that improves our detection of hidden messages within text. By employing the encoder of the Auto Encoder for feature extraction on input statistical information, we compress the original input into a vector containing crucial, yet compact, information. The training of the Auto-Encoders allows us to obtain the latent representation, which serves as the statistical features of the input text. This method draws inspiration from the foundational work of Hinton and Salakhutdinov with neural networks, providing a sophisticated approach to capturing the global representation of statistics within a latent space.

#### Fusion Component

In the realm of deep learning architectures for classification tasks, the accuracy of feature selection and integration is of utmost importance, which is why the fusion component merges Auto-Encoder-generated TF-IDF vectorized features with semantically rich features filtered through BERT inspired by the work of Guo et al. The fusion component aims to enhance the system's reliability by focusing on high-confidence, semantic information indicative of steganographic content. Statistical data, with its focus on quantifiable text attributes like TF-IDF scores and sentence lengths, excels at uncovering patterns or anomalies in distribution, while semantic data, enhanced by models like BERT, delves into text meaning, context, and linguistic relationships, capturing nuances beyond the reach of statistical analysis alone.

This approach is grounded in the identification of high-confidence features (those with activation values close to 0 or 1) as opposed to low-confidence features situated around the midpoint of 0.5. Such low-confidence features are frequently associated with classification inaccuracies. The hyperparameter \(\alpha\) sets the threshold for eliminating features around the low-confidence interval.

The inception of this fusion component was influenced by the significant improvements reported in Zhang et al.'s paper, where FSA accuracy was notably enhanced across two datasets consisting of news headlines and micro-blogs, compared to baseline models. By adopting and adapting this sophisticated component from financial sentiment analysis to steganalysis, our model not only overcomes the limitations inherent in relying solely on semantic or statistical analysis but also markedly improves its ability to uncover covert embedded steganography in news feeds, visible through an improvement of the F1-Score by 0.02.

#### Classifier

The classifier module has the crucial task of determining whether input text contains steganographic content. This module capitalizes on the power of the Bahdanau attention mechanism to refine the integration of representations from the original BERT-encoded text and the fused vector derived from the fusion component. The BERT-encoded text serves as the encoder's hidden state, and the fused features serve as the decoder's hidden state. Notably, the fusion information predominantly comprises semantic features when \(\alpha=0\), allowing the module to leverage self-attention for a refined analysis.

![Visualization of the Bahdanau Attention mechanism with *sl=sequence length*, influenced by Bahdanau et al., and Luong et al.](resources/figures/BahdanauAttention.png)

The utilization of the Bahdanau attention mechanism is essential for overcoming the limitations faced by traditional

 sequence-to-sequence (seq2seq) models, which typically encode input into a fixed-size context vector, leading to potential information loss, especially in longer texts. By dynamically adjusting the context vector at each decoding step, drawing from the rich semantic context provided by the BERT-encoded text and the nuanced statistical insights offered by the Auto-Encoded TF-IDF features, Bahdanau attention ensures a comprehensive analysis of the text. Alignment scores between the decoder's hidden state (Auto-Encoded TF-IDF features) and each of the encoder's hidden states (BERT-encoded text) are calculated to determine the focus needed on different parts of the input sequence. These scores are normalized into attention weights, which dictate the significance of each input element in predicting the current output. A final dense layer projects the aggregated feature vector onto an output vector, which is then transformed into a probability vector through the softmax function. This mechanism facilitates the classification of texts as either steganographic or normal, based on the predictive outcomes.

Leveraging the dynamic interplay of encoder and decoder states, our architecture not only achieves heightened sensitivity to subtle cues regarding contained stenographic embedded text but also capitalizes on the statistical depth provided by TF-IDF vectorized features. These features, integral to the fusion component, enrich the classifier's input with a layer of statistical analysis that complements the semantic richness extracted by BERT. This duality of statistical and semantic data allows for a more comprehensive examination of the text, enhancing the model's proficiency in detecting stenographic embedded content with greater accuracy and efficiency.

Central to this enhanced capability is the strategic integration of BERT's contextually rich embeddings with the discriminative power of TF-IDF vectorization, paired with the focused attention facilitated by the Bahdanau mechanism. Together, they form a robust detection mechanism, adept at uncovering the intricate patterns of hidden information.

#### Training

Optimization during the training phase was rigorously directed by the cross-entropy loss function, expressed as \(L = -\frac{1}{N} \sum_{i=1}^{N} \left[y \log \hat{y} + (1 - y) \log (1 - \hat{y})\right]\), where \(y\) symbolizes the text's true label, and \(\hat{y}\) its predicted label. Cross-entropy loss is chosen for model optimization due to its effectiveness in measuring the discrepancy between the model's predicted probabilities and the actual labels, penalizing incorrect predictions more severely when the model is confident. This attribute is vital in steganalysis to ensure precise and confident differentiation between steganographic and normal texts, enhancing the model's accuracy in identifying hidden information within texts.

Incorporated within this optimization process is a focus on batch learning, which facilitates efficient and effective model training by processing data in manageable subsets, thereby optimizing the use of computational resources and enabling the model to update its parameters more frequently. This approach not only accelerates the learning process but also contributes to a more dynamic adjustment of weights in response to the variability within the training data. Furthermore, the F1 score emerges as a critical metric for optimization, particularly due to its balanced consideration of both precision and recall. This metric is invaluable in steganalysis, where both the detection of steganographic content (recall) and the accurate identification of normal texts (precision) are equally important.

### Biggest Challenge: Size of the Datasets
Given the constraint of a small dataset, we employed various NLP augmentation techniques to enrich our training data artificially. One such method was back-translation, executed through the nltk toolkit, where texts were translated to another language and then back to the original, generating semantically similar but syntactically distinct sentences. Additionally, we explored synonym substitution, leveraging word embeddings and WordNet to identify and replace words with their synonyms, thus introducing lexical diversity without altering the underlying meaning. Unfortunately, the enhanced dataset did solely not serve as a good method to improve the model's performance.

To further adapt our model to the data scarcity, we utilized bootstrapping techniques. This involved generating multiple bootstrapped datasets by resampling the original training data with replacement, followed by iterative training and evaluation of the model on these varied samples. This approach aimed to find the model iteration that exhibited the best generalization across different representations of the training data. The rationale behind employing bootstrapping stemmed from its potential to enhance model reliability and generalization. Training models on small datasets often results in high variance and a propensity for overfitting. By training on multiple bootstrapped datasets, we aimed to establish a more generalized model performance, thus mitigating the risk of overfitting. We hoped that NLP augmentation and bootstrapping combined can have a positive impact on model performance, but unfortunately, it didn't.

As a response, we embarked on using a large dataset for training, crafted with the Synonym Hider and incorporating a diverse collection of news articles from a Kaggle dataset, originally compiled for a paper. This dataset spans a wide array of publications, from the New York Times to Breitbart, primarily covering the period from 2016 to July 2017. The dataset contains 3 files in CSV-format with approx. 100000 news articles. To provide a class balanced dataset, we split the news feed dataset into equally sized buckets of news feeds without steganography and news feeds modified with 2 different steganography approaches, the Synonym Hider and our Probability Order Hider. The extensive size of this dataset necessitated considerable computational resources and time for generating the steganographic examples and training, especially on a local machine.

The evaluation of our DL-based detector method on the Kaggle dataset yielded promising results, with Precision at 0.77, Recall at 0.82, and an F1 Score of 0.79. These outcomes hint that further refinements, such as implementing learning rate schedulers and weight decay, could potentially replicate the results of the original paper. However, despite these promising indicators, the DL-based detector's performance on the given dataset did not surpass random guessing. This discrepancy prompted us to consider not only the broad and generalized nature of the Kaggle dataset but also the possibility of a temporal shift affecting its alignment with our target dataset. The time frame of the Kaggle dataset might not fully encapsulate the temporal nuances or shifts that could be present in our dataset, contributing to the misalignment.

This experience underlines the critical importance of dataset compatibility and representativeness in model training and validation. While the Kaggle dataset provided a rich and politically diverse pool of articles, its general approach may not capture the nuances of more narrowly defined or topic-specific datasets. Nevertheless, the satisfactory scores achieved on our validation set underscore the model's potential effectiveness, given a sufficiently representative and ample dataset tailored to the specific steganalysis objectives.